\documentclass[12pt, A4]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{
	a4paper,
	left=15mm,
	right=15mm,
	top=25mm,
	bottom=20mm
}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=black,
	filecolor=magenta,      
	urlcolor=cyan,
	pdfpagemode=FullScreen,
}

\makeatletter
\newcommand*{\rom}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\renewcommand{\theenumi}{\roman{enumi}}
\newcommand{\indep}{\perp \!\!\! \perp}

\begin{document}
\begin{titlepage}
	\begin{center}
		\vspace*{5cm}
		\textbf{\Large Probability theory \rom{1} Facts}
		\\
		\vspace{1.5cm}
		\textbf{Taeyoung Chang}
		\vfill
		Textbook : R. Durrett $\ulcorner$ Probability : Theory and Examples $\lrcorner$ 5th edition
		\\
		\vspace{0.8cm}
		Last Update : \today
		\vspace*{3cm}
		\thispagestyle{empty}
	\end{center}
\end{titlepage}
\tableofcontents
\clearpage

\section{Probability Space}
\begin{itemize}
	\item[*] Sigma field $\mathcal{F}$ and event $A\in \mathcal{F}$
	\begin{itemize}
		\item A family of subsets of $\Omega$, named $\mathcal{F}$, is said to be a $\sigma$-field if \\(a) $\mathcal{F}$ contains $\Omega$ \quad (b) $\mathcal{F}$ is closed under taking complement \\(c) $\mathcal{F}$ is closed under taking countable union. \\(d) $\mathcal{F}$ is closed under taking countable intersection. : by (a),(b),(c)
		\item A $\sigma$-field $\mathcal{F}$ is usally called as an event space and\\ an element $A\in \mathcal{F}$ is said to be an event.
	\end{itemize}
	\item[*] Sigma field $\sigma(\mathcal{A})$ generated by a collection $\mathcal{A}$
	\begin{itemize}
		\item Given a collection $\mathcal{A}$ of subsets of $\Omega$, the smallest $\sigma$-field containing $\mathcal{A}$ is said to be a $\sigma$-field generated by $\mathcal{A}$ and denoted as $\sigma(\mathcal{A})$
	\end{itemize}
	\item[*] Borel field $\mathcal{B}(\mathbb{R})$
	\begin{itemize}
		\item Borel field $\mathcal{B}(\mathbb{R})$ is a $\sigma$-field on $\mathbb{R}$ generated by the family of all open subsets of $\mathbb{R}$.
	\end{itemize}
	\item Various collections generating $\mathcal{B}(\mathbb{R})$
	\begin{enumerate}
		\item Collections of open sets
		\item Collections of bounded open intervals
		\item Collections of bounded closed intervals
		\item Collections of bounded half open intervals
		\item Collections of open rays
		\item Collections of closed rays
	\end{enumerate}
    \item[*] Measure $\mu$ and Probability Measure $P$
    \begin{itemize}
    	\item A set function $\mu : (\Omega, \mathcal{F})\rightarrow \mathbb{R}$ is said to be a measure if \\ (a) $\mu$ is nonnegative \quad (b) $\mu$ is countably additive \\ (c) $\mu(\phi)=0$ : by convention
    	\item Additionally, if $\mu(\Omega)=1$, then it is called as probability measure and denoted as $P$ instead of $\mu$.
    \end{itemize}
    \item Elementary properties of measure
    \begin{enumerate}
    	\item Monotonicity
    	\item Subadditivity
    	\item Continuity from above
    	\item Continuity from below
    \end{enumerate}

\clearpage

	\item[*] $\pi$ system $\mathcal{P}$ and $\lambda$ system $\mathcal{L}$
	\begin{itemize}
		\item A collection $\mathcal{P}$ of subsets of $\Omega$ is a $\pi$-system if $\mathcal{P}$ is closed under taking intersection.
		\item A collection $\mathcal{L}$ of subsets of $\Omega$ is a $\lambda$-system if \\(a) $\mathcal{L}$ contains $\Omega$ \quad (b) $\mathcal{L}$ is closed under taking complement \\(c) $\mathcal{L}$ is closed under taking countable disjoint union.
	\end{itemize}
	\item A $\pi-\lambda$ system is a sigma field.
	\item $\pi-\lambda$ system thm
	\begin{itemize}
		\item let $\mathcal{P}, \mathcal{L}$ be a $\pi$-system and $\lambda$-system on $\Omega$ respectively. \newline If $\mathcal{P} \subset \mathcal{L}$ then $\sigma(\mathcal{P})\subset \mathcal{L}$
	\end{itemize}
	\item Checking two probability measures are the same
	\begin{itemize}
	\item If $P_1$ and $P_2$ are two probability measures on the same event space and $P_1=P_2$ on a $\pi$-system $\mathcal{P}$ then $P_1=P_2$ on $\sigma(\mathcal{P})$ 
	\item[$\square$] If two probability measures are the same on $\pi$-system generating a given event space then two probability measures are the same	
    \end{itemize}
\end{itemize}
\clearpage

\section{Random Variable}
\begin{itemize}
	\item[*] Random Variable $X$
	\begin{itemize}
		\item A function $X : (\Omega, \mathcal{F}) \rightarrow (\mathbb{R}, \mathcal{B}(\mathbb{R}))$ is said to be a random variable if \\$(X\in B)=X^{-1}(B)=\{\omega\in\Omega : X(\omega\in B)\}\in\mathcal{F} \hspace{0.4cm} \forall B\in\mathcal{B}(\mathbb{R}) $ 
	\end{itemize}
	\item Checking whether a mapping is a random variable is nearly same with checking whether a function is measurable.
	\item Elementary properties of random variable
	\begin{enumerate}
   	\item If $X$ is a r.v. then $c+X$ and $cX$ are r.v.'s for any real num. $c$
   	\item If $X$ and $Y$ are r.v.'s then $X+Y$ and $XY$ are r.v.'s
   	\item If $\{X_n\}$ is a random seq. then $\inf X_n$, $\sup X_n$, $\liminf X_n$, $\limsup X_n$ are all r.v.'s
   	\item If $X$ is a r.v. and $f$ is Borel measurable then $f(X)$ is a r.v.
	\end{enumerate}
    \item[*] Simple random variable
    \begin{itemize}
    	\item A random variable $X$ is called simple if $X$ takes a finite number of values.
    \end{itemize}
    \item If $X$ is a nonnegative random variable then $\exists$ a seq of nonnegative simple random variables $\{X_n\}$ s.t. $X_n\nearrow X$
    \begin{itemize}
		\item For each $n\in \mathbb{N}$, we can define $X_n$ by $$X_n=\sum_{k=1}^{n\cdot 2^n}\frac{k-1}{2^n}I\Big(\frac{k-1}{2^n}\leq X<\frac{k}{2^n}\Big) $$
	\end{itemize}
    \item[$\square$] If $X$ is a random varaible then $\exists$ a seq of simple random variables $\{X_n\}$ s.t. $X_n\rightarrow X$
    \item[*] $\sigma$-field $\sigma(X)$ generated by random variable $X$
    \begin{itemize}
    	\item $\sigma(X)=\{(X\in B) : B\in \mathcal{B}(\mathbb{R})\}$
    \end{itemize}
	\item[*] $\mathcal{G}$-measurable random variable
	\begin{itemize}
		\item For a sub $\sigma$-field $\mathcal{G}\subset \mathcal{F}$, a random variable X is said to be $\mathcal{G}$-measurable \\ if $(X\in B)\in \mathcal{G} \quad \forall B\in \mathcal{B}(\mathbb{R})$. Denote it as $X\in \mathcal{G}$
	\end{itemize}
    \item If $X$ and $Y$ are random variables and Y is $\sigma(X)$-measurable \\ then $\exists$ a Borel function $f$ s.t. $Y=f(X)$
\end{itemize}
\clearpage

\section{Distributions}
\begin{itemize}
	\item[*] Distribution function $F$
	\begin{itemize}
		\item A function $F : \mathbb{R}\rightarrow \mathbb{R}$ is said to be a distribution function if \\ (a) $F$ is monotone increasing \\(b) $F$ is right continuous \quad (c) $F$ has left limits. \\ (d) $F(x)\rightarrow 1$ as $x\rightarrow \infty$\hspace{0.2cm} \& $ F(x)\rightarrow 0$ as $x\rightarrow -\infty$
	\end{itemize}
	\item[*] The inverse of distribution function $F^{-1}(u)$
	\begin{itemize}
		\item let $F$ be a distribution function. For each $u\in {[0,1]}$, \\$F^{-1}(u)$ is defined as $F^{-1}(u)=\inf\{x\in \mathbb{R} : F(x)\geq u\}$
	\end{itemize}
	\item Properties of the inverse of distribution function
	\begin{enumerate}
		\item $u\mapsto F^{-1}(u)$ is monotone increasing
		\item $-\infty<F^{-1}(u)<\infty$ whenever $0<u<1$
		\item $F^{-1}(0)=-\infty$ and $F^{-1}(1)=\infty$ or $M$ for some $M<\infty$.
		\\ If  $F^{-1}(1)=M$ for some $M<\infty$ then $X$ is bounded above by $M$ a.s. where $X\sim F$
		\item $F^{-1}(u)\leq x \Leftrightarrow u\leq F(x) \hspace{1cm} \forall x\in \mathbb{R}, u \in [0,1]$ 
		\newline $F(x)<u \Leftrightarrow x<F^{-1}(u) \hspace{1cm} \forall x\in \mathbb{R}, u \in [0,1]$ 
		\item $u\leq F(F^{-1}(u))$ and $F(F^{-1}(u)-)\leq u \hspace{1cm} \forall u\in [0,1]$
		\item If F is continuous then $F(F^{-1}(u))=u \hspace{1cm} \forall u\in [0,1]$
	\end{enumerate} 
	\item[$\square$] If a r.v. $X\sim F$ and $\mathcal{U}\sim unif[0,1]$ then $F^{-1}(\mathcal{U})\overset{D}{=}X$
	\item[$\square$] If $X$ is a continuous r.v. with $X\sim F$ where $F$ is strictly increasing, then $F(X)\sim unif[0,1]$ \\ ($X$ is said to be continuous r.v. provided there is no point mass i.e. $P(X=x)=0 \; \forall \, x\in \mathbb{R}$)  
	\item[*] Probability Borel measure $\mu$
	\begin{itemize}
		\item Any probability measure $\mu$ on $(\mathbb{R}, \mathcal{B}(\mathbb{R}))$ is called as a probability Borel measure.
	\end{itemize}
	\item 1-1 correspondence of distribution function and prob. Borel measure
	\begin{itemize}
		\item For any distribution function F, \hspace{0.3cm} $\exists$ a unique prob. Borel measure $\mu$ s.t.\\ $\mu((-\infty, x])=F(x) \hspace{1cm} \forall x\in \mathbb{R}$
		\item For any prob. Borel measure $\mu$, \hspace{0.3cm} $F:\mathbb{R}\rightarrow \mathbb{R}$ defined by \newline $F(x)=\mu((-\infty, x])$ is a distribution function
	\end{itemize}
	\item If a function $F$ is monotone increasing and right-continuous satisfying $F(-\infty)=0$ and\\ $F(\infty)=1$ then $\exists$ a probability space $(\Omega, \mathcal{F}, P)$ and a random varaible $X$ s.t. \\ $P(X\leq x)=F(x) \hspace{0.2cm} \forall x\in \mathbb{R} $\quad i.e. Given $F$ is a distribution function for $X$ 
\end{itemize}
\clearpage

\section{Expected Value and Independence}
\begin{itemize}
	\item[*] Expected Value $E[X]$ / Integrability of a random variable $X$
	\begin{itemize}
		\item Given prob. space $(\Omega, \mathcal{F}, P)$, \quad $E[I_A]=\int I_A\, dP= P(A) \hspace{0.2cm} \forall A\in \mathcal{F}$
		\item For simple nonnegative random variable $X=\sum_{i=1}^{k}\alpha_iI_{A_i}$
		\\ $E[X]=\int X\, dP= \sum_{i=1}^{k}\alpha_iP(A_i)$
		\item For nonnegative random variable $X$, 
		\begin{equation*}
			\begin{split}
				E[X] &=\sup\{E[Z] : 0\leq Z\leq X \quad simple\} \\
				&=\lim_{n \to \infty}E[X_n]\quad  \forall simple\hspace{0.2cm} X_n\quad  s.t. \hspace{0.2cm} 0\leq X_n\nearrow X
			\end{split}
		\end{equation*}
		\item For a random variable $X$, 
		\begin{enumerate}
			\item $E[X]=E[X^+]-E[X^-]$
			\item $X$ is called integrable if $E|X|<\infty$ or if $E[X^+], E[X^-]<\infty$
		\end{enumerate}
		\end{itemize}
	\item[*] Independence of events $\{A_n\}$ \& collections of events $\{\mathcal{G}_n\}$
	\begin{itemize}
		\item $A\indep B\in \mathcal{F}$ if $P(A\cap B)=P(A)P(B)$ 
		\item $A_1,\cdots,A_n$ independent if $P(A_{i_1}\cap\cdots\cap A_{i_k})=P(A_{i_1})\cdots P(A_{i_k})$ \\ $\forall 1\leq i_1<i_2<\cdots<i_k\leq n$
		\item $\{A_n\}$ independent if $A_1,\cdots, A_m$ are independent $\forall m\in \mathbb{N}$
		\item For subcollections $\{\mathcal{G}_n\}\subset \mathcal{F} $, $\{\mathcal{G}_n\}$ are independent if $\{A_n\}$ are independent $\forall A_i\in \mathcal{G}_i $
	\end{itemize}
	\item If $\mathcal{G}_1,\cdots, \mathcal{G}_n\subset \mathcal{F}$ are independent and each $\mathcal{G}_i$ is a $\pi$-system \\ then $\sigma(\mathcal{G}_1),\cdots, \sigma(\mathcal{G}_n)$ are independent	
	\item[*] Independence of Random variables
	\begin{itemize}
		\item R.V.'s $\{X_n\}$ are independent if $\{\sigma(X_n)\}$ are independent 
	\end{itemize}
	\item For a collection $\mathcal{C}$ of subsets of $\mathbb{R}$ and a r.v. $X$, \newline $X^{-1}(\sigma(\mathcal{C}))=\sigma(X^{-1}(\mathcal{C}))$ where $X^{-1}(\mathcal{A})=\{(X\in A):A \in \mathcal{A} \}$ \#\ 1.3.1
	\item[$\square$] If $P(X_1\leq x_1, \cdots , X_n\leq x_n)=P(X_1\leq x_1)\cdots P(X_n\leq x_n)\hspace{0.5cm} \forall x_i\in \mathbb{R}$\\then $X_1, \cdots, X_n$ are independent
	\item[$\square$]If $(X_1,\cdots, X_n)$ has a joint density $f(x_1, \cdots, x_n)$ and $f$ can be written as $f(x)=g_1(x_1)\cdots g_n(x_n)$ where $g_k$'s are nonnegative and measurable, then $X_1,\cdots, X_n$ are independent  \quad \#\ 2.1.1
	\item[$\square$]If $X_1,\cdots,X_n$ are r.v.'s taking values in countable sets $C_1,\cdots, C_n$. \\ Then $P(X_1=x_1,\cdots X_n=x_n)=P(X_1=x_1)\cdots P(X_n=x_n)$ whenever $\forall x_i\in C_i$ \\ implies that $X_1, \cdots , X_n$ are independent. \quad \#\ 2.1.2
	\item If $X$ and $Y$ are independent and $f,g$ are Borel measurable functions, \\ then $f(X)$ and $g(Y)$ are independent \quad \#\ 2.1.6
	\item[*] limsup and liminf of seq of events. $\limsup A_n, \liminf A_n$
	\begin{itemize}
		\item $\limsup A_n=\bigcap_{n=1}^{\infty}\bigcup_{k\geq n}A_k$\quad
		$\liminf A_n=\bigcup_{n=1}^{\infty}\bigcap_{k\geq n}A_k$
	\end{itemize}
	\item[$\square$] $\limsup A_n=A_n$ infinitely often \quad $\liminf A_n=A_n$ all but finitely many n's
	\item[$\square$] $(\limsup A_n)^{C}=\liminf A_n^{C}$
	\item Borel Cantelli lemma
	\begin{itemize}
		\item For a seq. of events $\{A_n\}$, if $\sum_{n=1}^{\infty}P(A_n)<\infty$ then $P(A_n\hspace{0.2cm} i.o.)=0$
		\item For a seq. of independent events $\{A_n\}$, if $\sum_{n=1}^{\infty} P(A_n)=\infty$ then $P(A_n\hspace{0.2cm} i.o.)=1$
	\end{itemize}
	\item[$\square$] Given a seq. of independent events $\{A_n\}$, \newline
	$\sum_{n=1}^{\infty}P(A_n)<\infty \Leftrightarrow P(A_n\hspace{0.2cm} i.o.)=0$
	\quad and \quad $\sum_{n=1}^{\infty} P(A_n)=\infty \Leftrightarrow P(A_n\hspace{0.2cm} i.o.)=1$
	\newline This is called as Borel Cantelli 0-1 law
	\item[$\square$] Note that $P(A_n\hspace{0.2cm} i.o.)=0$ implies $P(A_n^{C}$ all but finitely many n's$)=1$
	\item[*] Almost sure convergence $X_n \rightarrow X \hspace{0.2cm} a.s.$
	\begin{itemize}
		\item $\{X_n\}$ converges \hspace{0.1cm}$a.s.$ if $P(C)=1$ where $C=\{\omega\in\Omega : X_n(\omega)\hspace{0.1cm} converges\}$
		\item $X_n \rightarrow X \hspace{0.2cm} a.s.$ if $P(X_n\rightarrow X)=1$
	\end{itemize}
	\item Classic results about interchanging limits and integrals(expectations)
	\begin{enumerate}
		\item {[Fatou's lemma]} If $X_n \geq 0 $ then $E[liminfX_n]\leq liminfE[X_n]$
		\newline If $X_n \geq 0$ and $X_n \rightarrow X \hspace{0.2cm} a.s.$ then $E[X]\leq liminfE[X_n]$
		\item {[MCT]} If $0\leq X_n \nearrow X \hspace{0.2cm} a.s.$ then $E[X_n]\nearrow E[X]$
		\newline If $X_n\nearrow X \hspace{0.2cm} a.s.$ and $\exists$ a r.v. Y s.t. $Y\leq X_n \hspace{0.2cm} \forall n \in \mathbb{N}$ and $E|Y|<\infty$ \\then MCT $E[X_n]\nearrow E[X]$ also holds
		\item {[DCT]} If $|X_n|\leq Y \hspace{0.2cm} a.s.\hspace{0.3cm} \forall n\in \mathbb{N}$ for some r.v. Y s.t. $E|Y|<\infty$ then $X_n\rightarrow X\hspace{0.2cm} a.s.$ implies that $E[X_n]\rightarrow E[X]$
		\item {[BCT]} If $|X_n|\leq B \hspace{0.2cm} a.s.\hspace{0.3cm} \forall n\in \mathbb{N}$ for some constant $B>0$ then $X_n\rightarrow X\hspace{0.2cm} a.s.$ implies that $E[X_n]\rightarrow E[X]$
	\end{enumerate}
    \item Almost sure convergence and convergence of expectation
    \begin{itemize}
    	\item $X_n\rightarrow X \hspace{0.2cm} a.s.$ implies $E[h(X_n)]\rightarrow E[h(X)]$ if the following conditions for continuous \\ $g$ and $h$ are satisfied.
    	\begin{enumerate}
    		\item $g>0$ (or $g\geq 0$ and $g(x)>0$ unless $|x|\leq M$ for some $M>0$)
    		\item $|h(x)|/g(x)\rightarrow 0$ as $|x|\rightarrow \infty$ (kind of ``h is dominated by g")
    		\item $\exists M>0$ s.t. $E[g(X_n)]\leq M \hspace{0.2cm} \forall n\in \mathbb{N}$
    	\end{enumerate}
    \end{itemize}
    \item [$\square$] If $p>1$ and $E|X_n|^p\leq M \hspace{0.2cm} \forall n\in \mathbb{N}$ for some $M>0$ \newline then $X_n\rightarrow X \hspace{0.2cm} a.s.$ implies $E[X_n]\rightarrow E[X]$
    \item Almost sure convergence is closed under continuous map \quad \#\ 1.3.3
    \begin{itemize}
    	\item If Borel measurable $f:\mathbb{R}\rightarrow\mathbb{R}$ is continuous then \\$X_n\rightarrow X \hspace{0.2cm} a.s.$ implies $f(X_n)\rightarrow f(X) \hspace{0.2cm} a.s.$  
    \end{itemize}
    \item Change of measure
    \begin{itemize}
    	\item For any Borel measurable function $f$ and a r.v. $X$, \newline if $f\geq 0$ or $E|f(X)|<\infty$ then $E[f(X)]$ is calculated by $$E[f(X)]=\int_{\Omega} f(X) dP=\int_{\mathbb{R}} f d(PX^{-1})$$ 
    \end{itemize}
    \item[$\square$] Change of measure and calculating probability 
    \begin{enumerate}
    	\item $P(X\in B)=E[I_B(X)] \hspace{0.3cm} \forall B\in \mathcal{B}(\mathbb{R})$
    	\item $P(f(X)\in A)=P(X\in f^{-1}(A)) \hspace{0.3cm} \forall A\in \mathcal{B}(\mathbb{R})$ \newline given $f$ is nonnegative or integrable
    \end{enumerate}
    \item[*] $\mathcal{L}_p$ space
    \begin{itemize}
    	\item $\mathcal{L}_0=$ The class of all random variables on $(\Omega, \mathcal{F}, P)$ \\ $\mathcal{L}_p=\{X\in \mathcal{L}_0:  E|X|^p< \infty\}\; (0<p<\infty)$ : normed vector space with $\|X\|_p=(E|X|^p)^{1/p}$
    \end{itemize}  
    \item Essential inequalities
    \begin{enumerate}
    	\item {[Markov]} $P(|X|\geq c)\leq \frac{1}{c}E|X| \hspace{0.3cm} \forall c>0$
    	\item {[H\"{o}lder]} Given $X\in \mathcal{L}_p$ and $Y\in \mathcal{L}_q$ with $p,q>1$ and $\frac{1}{p}+\frac{1}{q}=1$, \newline $E|XY|=\|XY\|_1\leq \|X\|_p\|Y\|_q$
    	\item {[Cauchy-Schwarz]} $(E|XY|)^2\leq E[X^2]E[Y^2]$ 
    	\item {[Jensen]} If $\phi : \mathbb{R}\rightarrow \mathbb{R}$ convex then $\phi(E[X])\leq E[\phi(X)]$ provided both expectations exist.
    	If $\phi$ strictly convex then $\phi(E[X])<E[\phi(X)]$ unless $X=E[X]\hspace{0.2cm} a.s.$ \quad \#\ 1.6.1
    \end{enumerate}
    \item If $X$ is a nonnegative r.v. then $E[X]=\int_{0}^{\infty}P(X>t)dt$
    \item Product measures of independent random variables
    \begin{itemize}
    	\item If $X_1, \cdots, X_n$ are independent with distributions $X_i \sim \mu_i \hspace{0.2cm} \forall i$, then a random vector $(X_1, \cdots X_n)$ has a distribution $\mu=\mu_1 \times \cdots \times \mu_n$
    \end{itemize}
	\item Fubini theorem
	\begin{itemize}
		\item Suppose $X, Y$ are independent r.v.'s with distributions $X\sim \mu,\, Y\sim \nu$.
		\newline If $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ is Borel measurable with $f\geq 0$ or $E|f(X,Y)|<\infty$ \\ then $E[f(X,Y)]=\iint f(x,y)\,d\mu(x)d\nu(y)= \iint f(x,y)\, d\nu(y)d\mu(x)$ 
		\item[$\square$] Suppose $X, Y$ are independent r.v's and $f,\, g: \mathbb{R}\rightarrow \mathbb{R}$ are Borel measurable functions.\\ If $f, g\geq 0$ or $E|f(X)|, \, E|g(Y)|<\infty$ then $E[f(X)g(Y)]=E[f(X)]E[g(Y)]$
	\end{itemize}
	\item[*] Tail $\sigma$-field $\mathcal{T}$
	\begin{itemize}
		\item The tail $\sigma$-field of events $\{A_n\}$ is $\mathcal{T}=\cap_{n=1}^\infty \sigma(A_n, A_{n+1},\cdots)$
	\end{itemize}
	\item Kolmogorov's 0-1 law
	\begin{itemize}
		\item Suppose $\{A_n\}$ is a seq of independent events and $\mathcal{T}$ is the tail $\sigma$-field of $\{A_n\}$. \\ If $A\in \mathcal{T}$ then $P(A)=0$ or $1$.
	\end{itemize}
	\item[*] Tail $\sigma$-field of random seq $\{X_n\}$
	\begin{itemize}
		\item The tail $\sigma$-field of random seq. $\{X_n\}$ is $\mathcal{T}=\cap_{n=1}^\infty \sigma(X_n, X_{n+1},\cdots)$\\
		$\surd\hspace{0.2cm} \sigma(X_n, X_{n+1},\cdots)=\sigma(\{(X_i\in B):B\in\mathcal{B}(\mathbb{R}), i=n,n+1,\cdots\})$
	\end{itemize}
	\item[$\square$] If $\{X_n\}$ is a seq of independent r.v.'s and $\mathcal{C}=\{\omega \in \Omega : X_n(\omega)\hspace{0.2cm} converges\}$ then $ \mathcal{C}$ lies in\\ a tail $\sigma$-field of $\{X_n\}$ and $P(\mathcal{C})=0$ or $1$
	\item Constructing independent random variables
	\begin{itemize}
		\item Given a finite number of distribution functions, $F_1, \cdots, F_n$, it is possible to construct independent random variables $X_1, \cdots, X_n$ with $X_i\sim F_i$ for each $i=1,\cdots,n$
	\end{itemize}
\end{itemize}
\clearpage

\section{Convergence in Probability}
\begin{itemize}
	\item[*] Convergence in probability $X_n \xrightarrow{P} X$
	\begin{itemize}
		\item $X_n \xrightarrow{P} X$ if $P(|X_n-X|\geq \epsilon)\rightarrow 0$ as $ n\rightarrow \infty \quad \forall \epsilon>0$
	\end{itemize}
	\item Equivalent condition with almost sure convergence
	\begin{itemize}
		\item $X_n \rightarrow X \hspace{0.2cm} a.s. \Leftrightarrow \hspace{0.2cm} \forall \epsilon>0, \hspace{0.1cm} P(\bigcup_{k\geq n}(|X_k-X|\geq \epsilon))\rightarrow 0 \hspace{0.1cm} as \hspace{0.2cm} n\rightarrow \infty$
		\newline $\Leftrightarrow \hspace{0.2cm} \forall \epsilon>0, \hspace{0.1cm} P(|X_n-X|>\epsilon \hspace{0.2cm} i.o.)=0$
	\end{itemize}
	\item Almost sure convergence is stronger than convergence in probability
	\begin{itemize}
		\item $X_n\rightarrow X \hspace{0.2cm} a.s.$ implies $X_n \xrightarrow{P} X$
		\item Converse does not hold. \\ (Counterexample)  $X_n\sim \text{Bern}(\frac 1n)\quad \forall \; n\in \mathbb{N}\; \Rightarrow X_n \xrightarrow{P} 0$ but $X_n\rightarrow 0 \;\, a.s.$ does not hold.
	\end{itemize}
	\item The limit is unique both for in almost sure sense or in probability sense
	\begin{itemize}
		\item $X_n\rightarrow X \hspace{0.2cm} a.s.$ and $X_n\rightarrow Y \hspace{0.2cm} a.s.$ then $X=Y \hspace{0.2cm} a.s.$
		\item $X_n \xrightarrow{P} X$ and $X_n \xrightarrow{P} Y$ then $X=Y \hspace{0.2cm} a.s.$ 
	\end{itemize} 
	\item Convergence in probability is closed under continuous map
	\begin{itemize}
		\item If Borel measurable $f : \mathbb{R}\rightarrow \mathbb{R}$ is continuous then $X_n \xrightarrow{P} X$ implies $f(X_n)\xrightarrow{P} f(X)$ 
	\end{itemize}
	\item[$\square$] If $X_n\xrightarrow{P} X$ and $Y_n\xrightarrow{P} Y$ then $X_n+Y_n\xrightarrow{P} X+Y$ and $X_nY_n\xrightarrow{P} XY$
	\item Convergence in $\mathcal{L}_p$ is stronger than convergence in probability
	\begin{itemize}
		\item If $E|X_n-X|^p\rightarrow 0$ for some $p\geq 1$ then $X_n\xrightarrow{P}X$
	\end{itemize}
	\item If $X_n\xrightarrow{P} X$ then $\exists$ a subseq $\{X_{n_k}\}$ of $\{X_n\}$ s.t. $\{X_{n_k}\}\rightarrow X \hspace{0.2cm} a.s.$
	\item Lemma about convergence of real seq
	\begin{itemize}
		\item For real seq. $\{x_n\}$,  if every subseq. of $\{x_n\}$ has a further subseq. converging to $x$, then $\{x_n\}$ converges to $x$
	\end{itemize}
	\item $X_n\xrightarrow{P} X$ if for every subseq. $\{X_{n_m}\}$ of $\{X_n\}, \exists$ a further subseq. \\ $\{X_{n_{m_k}}\}$ s.t. $X_{n_{m_k}}\rightarrow X \hspace{0.2cm} a.s.$	
	\item Revisiting classic convergence thm
	\begin{enumerate}
		\item {[Fatou's lemma]} If $X_n \geq 0$ and $X_n \xrightarrow{P} X$ then $E[X]\leq liminfE[X_n]$
		\item {[MCT]} If $0\leq X_n$ increasing and $X_n\xrightarrow{P}X$ then $E[X_n]\nearrow E[X]$
		\item {[DCT]} If $|X_n|\leq Y \hspace{0.2cm} a.s.\hspace{0.3cm} \forall n\in \mathbb{N}$ for some r.v. Y s.t. $E|Y|<\infty$ then $X_n\xrightarrow{P} X$ implies that $E[X_n]\rightarrow E[X]$
	\end{enumerate}
	\item If $X_n\xrightarrow{P} X$ and $f$ is continuous and bounded \newline then not only $f(X_n)\xrightarrow{P}f(X)$ but also $E[f(X_n)]\rightarrow E[f(X)]$
\end{itemize}

\clearpage
\section{Convergence in Distribution}
\begin{itemize}
	\item[*] Sub probability Borel measure
	\begin{itemize}
		\item a prob. Borel measure $\mu$ s.t. $\mu(\mathbb{R})\leq1$
	\end{itemize}
	\item[*] Weak convergence of sub prob. Borel measures $\mu_n\xrightarrow{w}\mu$
	\begin{itemize}
		\item For sub prob. Borel measures $\{\mu_n\}$ and $\mu$,\quad $\mu_n\xrightarrow{w}\mu$ if \\$\exists$ a dense $D\subset \mathbb{R}$ s.t. $\mu_n(a,b]\rightarrow\mu(a,b]$ as $n\rightarrow \infty \hspace{0.2cm} \forall a,b\in D$
	\end{itemize}
	\item Lemma about countable set and dense set
	\begin{itemize}
		\item If $D\subset \mathbb{R}$ and $D^C$ is countable then $D$ is dense in $\mathbb{R}$
	\end{itemize}
	\item[$\square$] Point mass set of a finite measure is at most countable.
	\begin{itemize}
		\item If $\mu$ is a measure on measurable space $(\mathcal{S},\mathcal{A})$ with $\mu(\mathcal{S})<\infty$ then\\ $E=\{x\in S : \mu(\{x\})>0\}$ is at most countable. 
	\end{itemize}
	\item Equivalent condition of weak convergence
	\begin{itemize}
		\item $\mu_n\xrightarrow{w}\mu \Leftrightarrow \hspace{0.2cm} \mu_n(a,b]\rightarrow \mu(a,b]$ whenever $\mu(\{a\})=\mu(\{b\})=0$
	\end{itemize}
	\item The limit is unique in weak convergence sense
	\begin{itemize}
		\item If $\mu_n\xrightarrow{w} \mu$ and $\mu_n\xrightarrow{w} \nu$ \hspace{0.1cm} then $\mu=\nu$\hspace{0.1cm} i.e.\hspace{0.1cm} $\mu(B)=\nu(B) \hspace{0.2cm} \forall B\in \mathcal{B}(\mathbb{R})$
	\end{itemize}
	\item[*] Weak convergence of distribution functions $F_n\Rightarrow F$
	\begin{itemize}
		\item $F_n\Rightarrow F$ if $\mu_n\xrightarrow{w} \mu$ where $\mu_n\sim\ F_n$ and $\mu\sim F$
	\end{itemize}
	\item Continuity set $C_F$ of a distribution function $F$ is dense in $\mathbb{R}$
	\item $F_n\Rightarrow F \Leftrightarrow F_n(x)\rightarrow F(x) \hspace{0.2cm}\forall x\in C_F$
	\item[*] Convergence in distribution $X_n\xrightarrow{D} X$
	\begin{itemize}
		\item $X_n\xrightarrow{D} X$ if $\mu_n\xrightarrow{w}\mu$ where $X_n\sim \mu_n$ and $X\sim \mu$
	\end{itemize}
	\item Convergence in probability is stronger than convergence in distribution
	\begin{itemize}
		\item $X_n\xrightarrow{P}X$ implies $X_n\xrightarrow{D} X$
	\end{itemize}
	\item For a constant $c\in \mathbb{R}$,\hspace{0.1cm} if $X_n\xrightarrow{D} c$ then $X_n\xrightarrow{P} c$
	\item Slutsky's thm
	\begin{enumerate}
		\item If $X_n\xrightarrow{D} X$ and $Y_n\xrightarrow{P} c$ where $c$ is a constant, then $X_n+Y_n\xrightarrow{D}X+c$ \quad \#\ 3.2.13
		\\Especially, if $X_n\xrightarrow{D}X$ and  $Z_n-X_n\xrightarrow{P}0$ then $Z_n\xrightarrow{D}X$
		\item If $X_n\xrightarrow{D} X$ and $\delta_n\xrightarrow{P} 0$ then $X_n\delta_n\xrightarrow{P} 0$
		\item If $X_n\xrightarrow{D} X$ and $Y_n\xrightarrow{P} c$ where $c$ is a constant, then $X_nY_n\xrightarrow{D} cX$ \quad \#\ 3.2.14
	\end{enumerate}
\clearpage
	\item Scheffe's thm
	\begin{itemize}
		\item Sps $\{X_n\}$ and $X$ have density functions $\{f_n\}$ and $f$ respectively. If $f_n\rightarrow f \hspace{0.2cm} \mu-a.e.$ where $\mu$ is Lebesgue measure, then $X_n\xrightarrow{D} X$
	\end{itemize}
	\item Skorohod's thm
	\begin{itemize}
		\item Suppose that $\{\mu_n\}$ and $\mu$ are prob. Borel measures s.t. $\mu_n \xrightarrow{w} \mu$. Then $\exists$ a prob. space $(\Omega^{'}, \mathcal{F}^{'}, P^{'})$ and r.v $\{X_n^{'}\}$ and $X^{'}$ s.t. $X_n^{'}\sim \mu_n$, $X^{'}\sim \mu$ and $X_n^{'}\rightarrow X^{'} \hspace{0.2cm} P^{'}-a.s.$ 
	\end{itemize}
	\item Continuous mapping thm
	\begin{itemize}
		\item If Borel measurable $f : \mathbb{R}\rightarrow \mathbb{R}$ satisfies $P(X\in D_f)=0$ where $D_f$ is discontinuity set of $f$ then $X_n \xrightarrow{D} X$ implies $f(X_n)\xrightarrow{D} f(X)$ 
	\end{itemize}
	\item If $X_n\xrightarrow{D} X$ then $E[g(X_n)]\rightarrow E[g(X)] \hspace{0.2cm} \forall$ continuous bounded $g$
	\item If $E[g(X_n)]\rightarrow E[g(X)] \hspace{0.2cm} \forall$ unif. continuous(or Lipschitz) bounded $g$ then $X_n\xrightarrow{D} X$
	\item $X_n\xrightarrow{D} X$ if for every subseq. $\{X_{n_m}\}$ of $\{X_n\}$, $\exists$ a further subseq. $\{X_{n_{m_k}}\}$ s.t.  $X_{n_{m_k}}\xrightarrow{D} X$
	\item The Portmanteau thm
	\begin{itemize}
		\item The followings are equivalent
		\begin{enumerate}
			\item $X_n\xrightarrow{D} X$
			\item $\liminf P(X_n\in G)\geq P(X\in G) \hspace{0.2cm} \forall$ open $G\subset \mathbb{R}$
			\item $\limsup P(X_n\in F)\leq P(X\in F) \hspace{0.2cm} \forall$ closed $F\subset \mathbb{R}$
			\item $P(X_n\in A)\rightarrow P(X\in A) \hspace{0.2cm} \forall A\in \mathcal{B}(\mathbb{R})$ with $P(X\in \partial A)=0$
		\end{enumerate}
	\end{itemize}
	\item Polya's thm \quad \#\ 3.2.9
	\begin{itemize}
		\item If $F_n\Rightarrow F$ and $F$ is continuous then $F_n\rightarrow F$ uniformly on $\mathbb{R}$
	\end{itemize}
	\item $\{X_n\}$ and $X$ are integer valued random variables. \\ Then $X_n\xrightarrow{D}X$ iff $P(X_n=m)\rightarrow P(X=m) \hspace{0.2cm} \forall m\in\mathbb{Z}$ \quad \#\ 3.2.12
	\item[*] Big $O_p$ and small $o_p$ notation
	\begin{itemize}
		\item $X_n=o_p(1)$ if $X_n\xrightarrow{P} 0$
		\item $X_n=O_p(1)$ if $\lim_{M \to \infty}\sup_{n}P(|X_n|>M)=0$
		\\ or equivalently $\forall \epsilon>0$, $\exists M_\epsilon \& N_\epsilon$ s.t. $P(|X_n|>M_\epsilon)<\epsilon \hspace{0.2cm} \forall n\geq N_\epsilon$
		\\$O_p(1)$ is also called as `stochasticallly bounded'
	\end{itemize}
	\item Elementary properties of Big $O_p$ and small $o_p$
	\begin{enumerate}
		\item $X_n=o_p(1), Y_n=o_p(1) \Rightarrow X_n+Y_n=o_p(1), X_nY_n=o_p(1)$
		\item $X_n=O_p(1), Y_n=O_p(1) \Rightarrow X_n+Y_n=O_p(1), X_nY_n=O_p(1)$
		\item $X_n=O_p(1), Y_n=o_p(1) \Rightarrow X_n+Y_n=O_p(1), X_nY_n=o_p(1)$
		\item $X_n\xrightarrow{D} X \Rightarrow X_n=O_p(1)$
	\end{enumerate}
\clearpage
	\item Helly's selection principle
	\begin{itemize}
		\item For a seq. $\{F_n\}$ of distribution functions, $\exists$ a subseq. $\{F_{n_k}\}$ and a distribution-like func. $F$ s.t. $F_{n_k}(x)\rightarrow F(x)$ as $k\rightarrow \infty \hspace{0.2cm} \forall x \in C_F$
	\end{itemize}
	\item[*] Tightness of seq. of distribution functions
	\begin{itemize}
		\item A seq. of distribution functions $\{F_n\}$ is called tight if \\ $\forall \epsilon>0$, $\exists M_\epsilon>0$ s.t. $limsup_n \hspace{0.1cm}1-F_n(M_\epsilon)+F_n(-M_\epsilon) \leq \epsilon$
	\end{itemize}
	\item For a seq. of distribution functions $\{F_n\}$, every subsequential limit is a distribution function iff $\{F_n\}$ is tight.
	\item Let $X_n \sim F_n \;\, \forall\;\, n\in \mathbb{N}$. If $X_n = O_p(1)$ then $\{F_n\}$ is tight.
	\item If $X_n = O_p(1)$ then there is a subsequence $\{X_{n_k}\}$ of $\{X_n\}$ and a random variable $X$ such that $X_{n_k} \xrightarrow{D} X$.
\end{itemize}
\bigskip

\section{Random Series}
\begin{itemize}
	\item Komogorov's inequality
	\begin{itemize}
		\item $\{X_n\}$ : a seq. of indep. r.v.'s with mean zero and finite variance. 
		\newline$$\forall \epsilon>0,\hspace{0.2cm} P(\max_{1\leq k \leq n}|S_k|\geq \epsilon)\leq \frac{1}{\epsilon^2}\sum_{k=1}^n \sigma_k^2$$
	\end{itemize}
	\item Convergence of random series
	\begin{itemize}
		\item $\{X_n\}$ : a seq. of indep. r.v.'s with mean zero and finite variance.  
		\[\sum_{n=1}^{\infty}\sigma_n^2<\infty \Rightarrow \sum_{n=1}^{\infty}X_n \hspace{0.2cm} converges\hspace{0.2cm} a.s.\] 
	\end{itemize}
	\item Etamadi's inequality
	\begin{itemize}
		\item $\{X_n\}$ : a seq. of independent r.v.'s. \newline $$\forall \epsilon>0, \hspace{0.2cm} P(\max_{1\leq k\leq n}|S_k|\geq3\epsilon) \leq 3\max_{1\leq k \leq n}P(|S_k| \geq \epsilon)$$
	\end{itemize}
	\item Levy's thm
	\begin{itemize}
		\item $\{X_n\}$ : a seq. of independent r.v.'s. If $S_n\xrightarrow{P}S$ then $S_n\rightarrow S$ a.s.
	\end{itemize}
	\item Lemma for Kolmogorovs's three series thm
	\begin{itemize}
		\item $\{X_n\}$ : a seq. of independent r.v.'s. \newline If $|X_n-E(X_n)| \leq A \hspace{0.2cm} a.s.$ for some $A>0 \hspace{0.2cm} \forall n\in \mathbb{N}$, then $$\forall \epsilon>0, \hspace{0.2cm} P(\max_{1\leq k \leq n}|S_k|\leq\epsilon)\leq
		\frac{(2A+4\epsilon)^2}{Var(S_n)}$$
	\end{itemize}
	\item[*] Eventual equivalence of random sequences $\{X_n\}\sim\{Y_n\}$
	\begin{itemize}
		\item Random seq. $\{X_n\}$ and $\{Y_n\}$ are said to be (eventually) equivalent \\if $\sum_{n=1}^{\infty}P(X_n\neq Y_n)<\infty$. Denote it as  $\{X_n\}\sim\{Y_n\}$
	\end{itemize}
	\item[$\square$] If $\{X_n\}\sim\{Y_n\}$ then $P(X_n=Y_n$ all but finitely many n's)$=1$
	\item Kolmogorov's three series thm
	\begin{itemize}
		\item $\{X_n\}$ : a seq. of independent r.v.'s. \\ For $A>0$, $\{Y_n\}$ is defined by $Y_n= X_nI(|X_n|\leq A)$  Then $\sum_n X_n$ converges a.s.\\ $\Leftrightarrow$ (a)$\sum_n P(|X_n|>A)<\infty$ (b)$\sum_n E(Y_n)$ converges. (c) $\sum_n Var(Y_n)<\infty $
	\end{itemize}
\end{itemize}

\bigskip
\section{Law of Large Numbers}
\begin{itemize}
	\item Lemma about eventually equivalent random sequences
	\begin{itemize}
		\item If random seq $\{X_n\}\sim\{Y_n\}$ and real seq. $\{a_n\}$ 
		s.t. $0<a_n\rightarrow\infty$ \\then for a random variable Z,
		\begin{enumerate}
			\item $\frac{1}{a_n}\sum_{j=1}^nX_j\rightarrow Z \hspace{0.2cm} a.s. \Leftrightarrow \frac{1}{a_n}\sum_{j=1}^nY_j\rightarrow Z \hspace{0.2cm} a.s.$
			\item $\frac{1}{a_n}\sum_{j=1}^nX_j\xrightarrow{P} Z \Leftrightarrow \frac{1}{a_n}\sum_{j=1}^nY_j\xrightarrow{P} Z$
		\end{enumerate}
	\end{itemize}
	\item Equivalent condition for integrability
	\begin{itemize}
		\item $\sum_n P(|X|\geq n)\leq E|X|\leq 1+ \sum_n P(|X|\geq n)$
		\item $E|X|<\infty \Leftrightarrow \sum_n P(|X|\geq n)<\infty$
	\end{itemize}
	\item Weak Law of Large numbers ; W.L.L.N
	\begin{itemize}
		\item $\{X_n\}$ i.i.d. random seq. with $E|X_1|<\infty$, $E(X_1)=\mu$ 
		 Then $\frac{1}{n}\sum_{j=1}^nX_j\xrightarrow{P}\mu$
	\end{itemize}
	\item Lemmas about convergence of real series
	\begin{itemize}
		\item {[Ces\`{a}ro mean]} If $x_n\rightarrow x $ then $\frac{1}{n}\sum_{j=1}^nx_j\rightarrow x$
		\item {[Kronecker's lemma]} $0<a_n\nearrow\infty$. If $\sum_{n=1}^{\infty} \frac{1}{a_n}x_n$ converges then $\frac{1}{a_n}\sum_{j=1}^nx_j \rightarrow 0 $
	\end{itemize}
	\item Strong Law of Large numbers ; S.L.L.N
	\begin{itemize}
		\item $\{X_n\}$ i.i.d. random seq. with $E|X_1|<\infty$, $E(X_1)=\mu$. 
		Then $\frac{1}{n}\sum_{j=1}^nX_j\rightarrow\mu \hspace{0.2cm} a.s.$
	\end{itemize}
	\item Strong Law holds even if the mean is infinite
	\begin{itemize}
		\item $\{X_n\}$ i.i.d. random seq. with $E(X_1^+)=\infty$. $E(X_1^{-})<\infty$. 
		\\Then $\frac{1}{n}\sum_{j=1}^nX_j\rightarrow \infty \hspace{0.2cm} a.s.$
		\item $\{X_n\}$ i.i.d. random seq. with $E(X_1^+)<\infty$. $E(X_1^{-})=\infty$.
		\\Then $\frac{1}{n}\sum_{j=1}^nX_j\rightarrow -\infty \hspace{0.2cm} a.s.$
	\end{itemize}
	\item If $\{X_n\}$ i.i.d. random seq. with $E|X_1|=\infty$ then\\
	(a)$P(|X_n|\geq n \hspace{0.2cm} i.o)=1\hspace{0.2cm}$
	(b)$P(\lim_{n \to \infty} \frac{1}{n}S_n \ exists \ and \ finite)=0$
	\item Glivenko Cantelli thm
	\begin{itemize}
		\item $\{X_n\}$ i.i.d. random seq. $X_1\sim F$. Empirical distribution function is defined as $F_n(x)=\frac{1}{n}\sum_{i=1}^{n}I(X_i\leq x) \hspace{0.2cm} \forall x\in \mathbb{R}$ for each $n \in \mathbb{N}$
		\newline Then $F_n \rightrightarrows F \hspace{0.2cm} a.s.$ \hspace{0.1cm} i.e. \hspace{0.1cm} $sup_{x\in \mathbb{R}}|F_n(x)-F(x)|\rightarrow 0 \hspace{0.2cm} a.s.$
	\end{itemize}
	\item $\{X_n\}$ i.i.d. random seq. with $E|X_1|=\infty$. \newline $\{a_n\}$ positive real seq s.t. $a_n/n$ is monotone increasing. Then \\
	$limsup_{n}\frac{|S_n|}{a_n}=\begin{cases}0\quad a.s.  & 
		\quad if  \ \sum_n P(|X_1|\geq a_n) <\infty
		\\\infty \quad a.s.& \quad if \ \sum_n P(|X_1|\geq a_n) =\infty\end{cases}$
	\item Convergence rate of random series
	\begin{itemize}
		\item $\{X_n\}$ i.i.d. random seq. with mean zero and finite variance. Then $$\forall \epsilon>0,\hspace{0.2cm} \frac{S_n}{\sqrt{n}(\log n)^{\frac{1}{2}+\epsilon}}\rightarrow0 \hspace{0.3cm} a.s.$$
	\end{itemize}
	\item The law of iterated logarithm(L.I.L)
	\begin{itemize}
		\item  $\{X_n\}$ i.i.d. random seq. with mean zero and finite variance $\sigma^2$. Then 
		\begin{align*}
			&\limsup_n \frac{S_n}{\sqrt{2n\log\log n}\sigma}=1\;\,a.s. \\
			&\liminf_n \frac{S_n}{\sqrt{2n\log\log n}\sigma}=-1\;\,a.s. \\
			&\limsup_n \frac{|S_n|}{\sqrt{2n\log\log n}\sigma}=1\;\,a.s. \\
			\forall \; \varepsilon>0,\quad &P(S_n\geq(1+\varepsilon)\sigma\sqrt{2n\log\log n}\;\,i.o.)=0 \\
			&P(S_n\leq-(1+\varepsilon)\sigma\sqrt{2n\log\log n}\;\,i.o.)=0 	
		\end{align*}
	\end{itemize}
\end{itemize}

\clearpage

\section{Characteristic function}
\begin{itemize}
	\item[*] Characteristic function
	\begin{itemize}
		\item A char. func. $\psi$ corresponding to a prob. Borel measure $\mu$ is
		\begin{equation*}
			\begin{split}
				\psi(t) &=\int e^{itx}\, d\mu(x) \\
				&= \int \cos tx\,d\mu(x)+i\int \sin tx\,d\mu(x)
			\end{split}
		\end{equation*}
		\item A char. func. of a r.v. $X\sim\mu$ is $\psi_X(t)=E[e^{itX}]=\int e^{itx}\,d\mu(x)$ 
	\end{itemize}
	\item Elementary properties of characteristic functions
	\begin{enumerate}
		\item $\psi(0)=1$
		\item $|\psi(t)|\leq 1 \hspace{0.2cm} \forall t\in \mathbb{R}$
		\item $\sup_{t\in \mathbb{R}}|\psi(t+h)-\psi(t)|\rightarrow 0$ as $h\rightarrow 0$. \hspace{0.2cm}$\psi$ is uniformly continuous
		\item $\psi_{aX+b}(t)=e^{itb}\psi_{X}(at) \hspace{0.2cm} \forall a,b,\in \mathbb{R}$
		\item $\psi(-t)=\overline{\psi}(t)$. \hspace{0.2cm} If $X\sim \psi$ then $-X\sim \overline{\psi}$
	\end{enumerate}
	\item Additional properties of characteristic functions
	\begin{enumerate}
		\item If $X_1 \indep X_2$ and $X_1\sim \psi_1, X_2\sim \psi_2$ then $X_1+X_2\sim \psi_1\psi_2$ \newline If $X_1,\cdots, X_n$ independent with $X_i\sim \psi_i$ then $\sum_{i=1}^{n}X_i\sim \prod_{i=1}^{n}\psi_i$
		\item If $\psi_1, \cdots \psi_n$ are char. func.s then $\prod_{i=1}^{n}\psi_i$ is also a char. function.
		\item If $\psi_1, \cdots \psi_n$ are char. func.s with $\psi_i \sim \mu_i$ then $\sum_{i=1}^{n}\lambda_i\psi_i$ is also a char. function given $\lambda_i>0$ and $\sum_{i=1}^{n}\lambda_i=1$
		\item If $\psi$ is a char. func then $Re(\psi)$ and $|\psi|^2$ are also char. functions.
	\end{enumerate}
	\item[$\square$] Characteristic function of $X\sim N(0,1)$ is $exp(-\frac{1}{2}t^2)$\quad  $X\sim N(\mu, \sigma)$ is $exp(i\mu t-\frac{1}{2}\sigma^2 t^2)$
	\newline Characteristic function of $X\sim Poi(\lambda)$ is $exp(\lambda(e^{it}-1))$
	\item The Inversion formula
	\begin{itemize}
		\item If $\psi$ is a char. func. corresponding to a distribution $\mu$, then whenever $a<b$,\\ the equation below holds true. 
		\[\frac{1}{2}\{\mu(a,b]+\mu[a,b)\}=\lim_{T \to \infty}\frac{1}{2\pi}\int_{-T}^{T} \frac{e^{-ita}-e^{-itb}}{it}\psi(t)\,dt \]
	\end{itemize}
	\item If two distributions $\mu$ and $\nu$ corresponds to same characteristic function, then $\mu$ and $\nu$ are indeed the same distributions. In this sense, characteristic functions uniquely determine the distribution.
	\item Suppose $\psi$ is a char. func. corresponding to $\mu\sim F$. \newline If $\int_{-\infty}^{\infty}|\psi(t)|dt<\infty$ then $F$ is diff.able and the density $f$ is derived as \[f(x)=F'(x)=\frac{1}{2\pi}\int_{-\infty}^{\infty}e^{-itx}\psi(t)\,dt\]
	\item Suppose $\psi$ is a char. func. corresponding to $\mu$. For each $x_0\in \mathbb{R}$, $$\mu(\{x_0\})=\lim_{T \to \infty}\frac{1}{2T}\int_{-T}^{T}e^{-itx_0}\psi(t)\,dt$$
	\item Suppose $\psi$ is a char. func. corresponding to $\mu$.
	$$\sum_{x}{[\mu(\{x\})]^2}=\lim_{T \to \infty}\frac{1}{2T}\int_{-T}^{T}|\psi(t)|^2\,dt$$
	\item Suppose $\psi$ is a char. func. corresponding to $\mu\sim F$. If $F$ is diff.able with density $f$ then $\lim_{t \to \infty} \psi(t)=0$
	\item[$\square$] If a distribution $\mu$ has a density $f$ where $\mu\sim F$ and $f=F'$ then $\mu$ has no point mass. i.e.\hspace{0.2cm} $\mu(\{x\})=0 \hspace{0.2cm} \forall x\in \mathbb{R}$ \quad \#\ 3.3.3
	\item Lemma for continuity thm
	\begin{itemize}
		\item $\mu$ : a sub prob. Borel measure. $\psi(t)=\int e^{itx}d\mu(x)$. Then $\forall \delta>0,$ $$\mu{[-\delta. \delta]}\geq \frac{\delta}{2}\left|\int_{-2/\delta}^{2/\delta}\psi(t)\,dt\right|-1$$
	\end{itemize}
	\item Levy's Continuity thm
	\begin{itemize}
		\item let $\{\mu_n\}$ be a seq. of distributions and $\mu_n \sim \psi_n \hspace{0.2cm} \forall n\in \mathbb{N}$.
		\newline If $\psi_n \rightarrow \psi$ pointwise and $\psi$ is continuous at zero, \newline then $\psi$ is a char. func. correspoding to $\mu$ and $\mu_n \xrightarrow{w} \mu$
		\item let $\{\mu_n\}$ be a seq. of distributions and $\mu_n \sim \psi_n \hspace{0.2cm} \forall n\in \mathbb{N}$.
		\newline If $\mu_n\xrightarrow{w} \mu$ and $\mu \sim \psi$ then $\psi_n \rightarrow \psi$ pointwise.
	\end{itemize}
	\item Suppose $X,Y$ i.i.d. with mean zero, variance one. \\If $X+Y\indep X-Y$ then $X,Y$ are normal r.v.'s.
	\item Suppose $\{X_n\}$ and $\{Y_n\}$ are independent and $X\indep Y$. \\If $X_n\xrightarrow{D} X$ and $Y_n\xrightarrow{D}Y$ then $X_n+Y_n\xrightarrow{D}X+Y$ \quad \#\ 3.3.8 
\end{itemize}

\bigskip
\section{Central Limit Theorem}
\begin{itemize}
	\item Lemma about convergence of sequence and exponential
	\begin{enumerate}
		\item $(1+\frac{a_n}{n})^n \rightarrow e^a$ if $a_n\rightarrow a$
		\quad $(1+c_n)^n \rightarrow e^c$ if $nc_n\rightarrow c$
		\item $(1+\frac{a_n}{\lambda_n})^{\lambda_n} \rightarrow e^a$ if $a_n\rightarrow a, \hspace{0.2cm} 0<\lambda_n \nearrow \infty$
		\newline $(1+c_n)^{\lambda_n}\rightarrow e^c$ if $\lambda_nc_n\rightarrow c, \hspace{0.2cm} 0<\lambda_n\nearrow \infty$
	\end{enumerate}
	\item Lemma about taylor expansion error term of $e^{ix}$
	$$\left|e^{ix}-\sum_{k=0}^{n}\frac{(ix)^k}{k!}\right|\leq min\left\{\frac{|x|^{n+1}}{(n+1)!}, \frac{2|x|^n}{n!}\right\}\quad \forall x\in\mathbb{R} \hspace{0.2cm} \forall n\in \mathbb{N}$$
	\item Lemma about taylor expansion of characteristic function
	\begin{enumerate}
		\item If $E|X|^n<\infty$ then $$\left|E\left[e^{itx}-\sum_{k=0}^{n}\frac{(itx)^k}{k!}\right]\right|\leq|t|^nE[min\{|t||X|^{n+1}, 2|X|^n\}]$$
		$$\psi(t)=\sum_{k=0}^{n}\frac{i^kE(X^k)}{k!}t^k+o(|t|^n)\quad as\hspace{0.2cm} t\rightarrow 0$$
		\item Especially if $E[X]=0$ and $E[X^2]=\sigma^2<\infty$, then
		$$\psi(t)=1-\frac{\sigma^2}{2} t^2 + o(t^2) \quad as\hspace{0.2cm} t\rightarrow 0$$
	\end{enumerate}
	\item Central Limit Thm
	\begin{itemize}
		\item $\{X_n\}$ i.i.d. random seq. $E[X_1]=\mu, Var(X_1)=\sigma^2<\infty$. Then $$\frac{S_n-n\mu}{\sigma\sqrt{n}}\xrightarrow{D}N(0,1)\quad i.e.\hspace{0.3cm} \frac{S_n-E[S_n]}{\sqrt{Var(S_n)}}\xrightarrow{D}N(0,1)$$
	\end{itemize}
	\item[*] Lindberg's condition
	\begin{itemize}
		\item $\{X_{nk}:k=1,\cdots, r_n\}$ : row-wise independent double array of r.v.'s with mean zero and finite variance.  $\{X_{nk}\}$ is said to be satisfying Lindberg's condition if
		$$\epsilon>0, \quad 
		\lim_{n \to \infty}\frac{1}{\mathcal{D}_n^2}\sum_{k=1}^{r_n}\int_{(|X_{nk}|\geq \epsilon\mathcal{D}_n)} X_{nk}^2\,dP=0$$
		\newline where $S_n=X_{n1}+\cdots+X_{nr_n}$ and $\mathcal{D}_n^2=Var(S_n)=\sigma_{n1}^2+\cdots+\sigma_{nr_n}^2$
	\end{itemize}
	\item Lemma about complex numbers
	\begin{enumerate}
		\item $\left|\prod_{i=1}^{n}z_i-\prod_{i=1}^{n}w_i\right|\leq \sum_{i=1}^{n}|z_i-w_i|$ if $|z_i|,|w_i|\leq 1$
		\item $|e^z-(1+z)|\leq \frac12 e^c|z|^2$ whenever $|z|\leq c$
		\newline Especially, $|e^z-(1+z)|\leq |z|^2$ if $|z|\leq 1/2$
		\item $|e^z|=e^{Re(z)}\leq e^{|z|} \quad \forall z\in \mathbb{C}$
		\item For $\{z_n\}\in \mathbb{C}$, $e^{z_n}\rightarrow e^z \Rightarrow e^{Re(z_n)}\rightarrow e^{Re(z)}$
	\end{enumerate}
	\item Feller's Thm
	\begin{itemize}
		\item $\{X_{nk}:k=1,\cdots, r_n\}$ : row-wise independent double array of r.v.'s with mean zero and finite variance. $S_n=X_{n1}+\cdots+X_{nr_n}$ and $\mathcal{D}_n^2=Var(S_n)=\sigma_{n1}^2+\cdots+\sigma_{nr_n}^2$
		\newline Lindberg's condition is satisfied if and only if \newline(a) $S_n/\mathcal{D}_n \xrightarrow{D} N(0,1)$ \hspace{0,2cm}(b) $\frac{1}{\mathcal{D}_n^2}\max_{1\leq k\leq r_n}\sigma_{nk}^2\rightarrow0$ as $n\rightarrow \infty$
	\end{itemize}
	\clearpage

	\item[*] Lyapunov condition
	\begin{itemize}
		\item $\{X_{nk}:k=1,\cdots, r_n\}$ : row-wise independent double array of r.v.'s with mean zero.
		\newline $\{X_{nk}\}$ is said to be satisfying Lyapunov condition if $\exists \hspace{0.1cm}\delta>0$ s.t.
		\begin{enumerate}
			\item $E|X_{nk}|^{2+\delta}<\infty$
			\item $\lim_{n \to \infty}\frac{1}{\mathcal{D}_n^{2+\delta}}\sum_{k=1}^{r_n}E|X_{nk}|^{2+\delta}=0$
		\end{enumerate}
	\end{itemize}
	\item Lyapunov condition is stronger than Lindberg's condition
	\item Poisson approximation of binomial random variable
	\begin{enumerate}
		\item $\{X_{nk}:k=1,\cdots, n\}$ : row-wise independent double array of Bernoulli r.v.'s. \\ $p_{nk}=P(X_{nk}=1)$. 
		If (a) $\sum_{k=1}^{n}p_{nk}\rightarrow \lambda$ where $\lambda\in(0,\infty)$ and (b) $\max_{1\leq k \leq n} p_{nk}\rightarrow 0$
		\\ then $S_n=X_{n1}+\cdots+X_{nn}\xrightarrow{D}Poi(\lambda)$
		\item  $\{X_{nk}:k=1,\cdots, n\}$ : row-wise independent double array of r.v.'s having nonnegative integer values. \quad $p_{nk}=P(X_{nk}=1)$ and  $\epsilon_{nk}=P(X_{nk}\geq2)$ \\
		If (a) $\sum_{k=1}^{n}p_{nk}\rightarrow \lambda$ where $\lambda\in(0,\infty)$ \quad (b) $\max_{1\leq k \leq n} p_{nk}\rightarrow 0$ \\and (c) $\sum_{k=1}^{n}\epsilon_{nk}\rightarrow 0$ (which means $X_{nk}$ is nearly Bernoulli r.v.)
		\\then $S_n=X_{n1}+\cdots+X_{nn}\xrightarrow{D}Poi(\lambda)$
	\end{enumerate}
\end{itemize}


\end{document}

